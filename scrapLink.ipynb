{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Scraping from some base url of USF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_urls = [\n",
    "    \"https://www.usf.edu\",\n",
    "    \"https://www.stpetersburg.usf.edu\",\n",
    "    \"https://www.sarasotamanatee.usf.edu\",\n",
    "    \"https://www.usf.edu/about-usf/site-map.aspx\",\n",
    "    \"https://usf.campusdish.com\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_links = set()\n",
    "queue = deque()\n",
    "def get_links_from_url(url):\n",
    "    links = []\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            # Check if the href is a full URL (either http or https) \n",
    "            if href.startswith((\"https://\")) and 'usf.edu' in href:\n",
    "                full_url = href\n",
    "            # If not, try to identify the correct base URL\n",
    "            else:\n",
    "                for base in base_urls:\n",
    "                    if href.startswith(base):\n",
    "                        full_url = urljoin(base, href)\n",
    "                        break\n",
    "                else:\n",
    "                    # Default to current page's base URL if not matched with known base URLs\n",
    "                    full_url = urljoin(url, href)\n",
    "            \n",
    "            if full_url and full_url not in visited_links and full_url.startswith((\"https://\")):\n",
    "                links.append(full_url)\n",
    "                visited_links.add(full_url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "    return links\n",
    "\n",
    "    \n",
    "\n",
    "def deep_scrape(start_urls):\n",
    "    all_links = []\n",
    "    \n",
    "    for url in start_urls:\n",
    "        # Add the starting URL to the queue\n",
    "        queue.append(url)\n",
    "        visited_links.add(url)\n",
    "    \n",
    "    while queue:\n",
    "        current_url = queue.popleft()\n",
    "        links = get_links_from_url(current_url)\n",
    "        all_links.extend(links)\n",
    "        \n",
    "        # Add unvisited links to the queue\n",
    "        for link in links:\n",
    "            if link not in visited_links:\n",
    "                queue.append(link)\n",
    "                visited_links.add(link)\n",
    "                \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls1 = deep_scrape(base_urls)\n",
    "print(\"The number of links found is: \", len(urls1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the second time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://usf.campusdish.com/ITSupport: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "urls2 = deep_scrape(urls1)\n",
    "print(\"The number of links found is: \", len(urls2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total of links scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4244, 4547)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_urls = urls1.extend(urls2)\n",
    "print(\"The number of unique links found is: \", len(set(all_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(urls, test_cases):\n",
    "    test = test_cases\n",
    "    numTestCase = 0\n",
    "    for url in test:\n",
    "        if url in urls:\n",
    "            numTestCase += 1\n",
    "        else:\n",
    "            print(f\"Test case failed: {url}\")\n",
    "    print(f\"{numTestCase} out of {len(test)} test cases passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = ['https://www.usf.edu/about-usf/accreditation.aspx',\n",
    "'https://www.usf.edu/about-usf',\n",
    "'https://www.sarasotamanatee.usf.edu/index.aspx',\n",
    "'https://pandemic-response-research.net/',\n",
    "'https://www.usf.edu/about-usf/accreditation.aspx',\n",
    "'https://www.usf.edu/ods/accreditation/accreditation.aspx',\n",
    "'https://www.usf.edu/about-usf',\n",
    "'https://www.usf.edu/academics/colleges.aspx',\n",
    "'https://www.usf.edu/engineering',\n",
    "'https://www.usf.edu/academics/colleges.aspx',\n",
    "'https://usf.campusdish.com/ITSupport',\n",
    "'http://eds.b.ebscohost.com/eds/results?vid=0&sdb=edspub&tid=3000EP%20&bquery=JN+B*+OR+JN+THE+B*+OR+JN+DER+B*+OR+JN+DIE+B*+OR+JN+DAS+B*+OR+JN+LAS+B*+OR+JN+LOS+B*+OR+JN+LES+B*+OR+JN+EL+B*+OR+JN+IL+B*+OR+JN+LA+B*+OR+JN+LE+B*&bdata=JmRiPWVkc3B1YiZ0eXBlPTEmc2l0ZT1lZHMtbGl2ZQ%3d%3d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case failed: http://eds.b.ebscohost.com/eds/results?vid=0&sdb=edspub&tid=3000EP%20&bquery=JN+B*+OR+JN+THE+B*+OR+JN+DER+B*+OR+JN+DIE+B*+OR+JN+DAS+B*+OR+JN+LAS+B*+OR+JN+LOS+B*+OR+JN+LES+B*+OR+JN+EL+B*+OR+JN+IL+B*+OR+JN+LA+B*+OR+JN+LE+B*&bdata=JmRiPWVkc3B1YiZ0eXBlPTEmc2l0ZT1lZHMtbGl2ZQ%3d%3d\n",
      "11 out of 12 test cases passed.\n"
     ]
    }
   ],
   "source": [
    "test(all_urls, test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the data to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_urls(all_urls):\n",
    "    with open(\"urls.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_urls, f)\n",
    "        print(f\"Saved {len(all_urls)} URLs to urls.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4547 URLs to urls.pkl\n"
     ]
    }
   ],
   "source": [
    "export_urls(all_urls)\n",
    "print(f\"Saved {len(all_urls)} URLs to urls.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
